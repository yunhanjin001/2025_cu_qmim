{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "REnkGXC1wgG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9622e3b-3d73-4094-9a5b-55d5be544021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# file path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "xlsx_path = \"/content/drive/MyDrive/Dataset.xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "px_all = pd.read_parquet(\"/content/drive/MyDrive/price_metrics.parquet\")\n",
        "px_all.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "R2XdS1o8zgi6",
        "outputId": "53554c46-5433-4b54-bf19-73ae326ff150"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/price_metrics.parquet'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3113301663.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpx_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/price_metrics.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpx_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/price_metrics.parquet'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract PX_LAST only (MultiIndex columns)\n",
        "px = px_all.xs(\"PX_LAST\", axis=1, level=\"metric\")\n",
        "# Resample to month end\n",
        "px_m = px.resample(\"M\").last()\n",
        "px_m"
      ],
      "metadata": {
        "id": "rm85UyYI0j5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute 6-1 and 12-1 momentum\n",
        "mom_6_1 = np.log(px_m.shift(1)) - np.log(px_m.shift(7))\n",
        "\n",
        "# Winsorize\n",
        "def winsorize(row, lower=0.01, upper=0.99):\n",
        "    if row.isna().all():\n",
        "        return row\n",
        "    lo, hi = row.quantile([lower, upper])\n",
        "    return row.clip(lo, hi)\n",
        "\n",
        "mom6_w = mom_6_1.apply(winsorize, axis=1)\n"
      ],
      "metadata": {
        "id": "UIDnWxJU1Ttz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Z-score\n",
        "mom6_z  = mom6_w.sub(mom6_w.mean(axis=1), axis=0).div(mom6_w.std(axis=1), axis=0)\n",
        "\n",
        "\n",
        "# Baseline portfolio\n",
        "def build_positions(signal, long_q=0.8, short_q=0.2):\n",
        "    pos = pd.DataFrame(index=signal.index, columns=signal.columns)\n",
        "\n",
        "    for dt, row in signal.iterrows():\n",
        "        r = row.rank(pct=True)\n",
        "        pos.loc[dt] = (r >= long_q).astype(int) - (r <= short_q).astype(int)\n",
        "\n",
        "    return pos\n",
        "\n",
        "pos6_m  = build_positions(mom6_z)\n",
        "\n",
        "\n",
        "# Compute daily return\n",
        "daily_ret = px.pct_change()\n",
        "\n",
        "# In-sample window\n",
        "px_bt = px.loc[\"2010-01-01\":\"2020-12-31\"]\n",
        "daily_ret_bt = daily_ret.loc[\"2010-01-01\":\"2020-12-31\"]\n",
        "\n",
        "pos6_d  = pos6_m.reindex(daily_ret_bt.index).ffill()\n",
        "\n",
        "\n",
        "# Compute strategy returns\n",
        "strategy6 = (pos6_d.shift(1) * daily_ret_bt).mean(axis=1).fillna(0)\n",
        "\n",
        "\n",
        "# Cum returns\n",
        "cum6 = (1 + strategy6).cumprod()\n",
        "\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(cum6, label=\"6-1 Momentum\")\n",
        "plt.title(\"Baseline Momentum Backtest (2010â€“2020)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "98rqDWmF1o8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factors_std = pd.read_parquet(\"/content/drive/MyDrive/factors_std.parquet\")"
      ],
      "metadata": {
        "id": "2BKPbubj-5td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_metrics = [\n",
        "\n",
        "    # VALUE\n",
        "    \"PE_RATIO\",\n",
        "    \"PX_TO_BOOK_RATIO\",\n",
        "    \"PX_TO_SALES_RATIO\",\n",
        "    \"CURRENT_EV_TO_T12M_EBITDA\",\n",
        "    \"FREE_CASH_FLOW_YIELD\",\n",
        "    \"EQY_DVD_YLD_12M\",\n",
        "\n",
        "    # QUALITY\n",
        "    \"EBITDA_MARGIN\",\n",
        "    \"GROSS_MARGIN\",\n",
        "    \"OPER_MARGIN\",\n",
        "    \"PROF_MARGIN\",\n",
        "    \"RETURN_ON_ASSET\",\n",
        "\n",
        "    # LEVERAGE\n",
        "    \"TOT_DEBT_TO_EBITDA\",\n",
        "    \"TOT_DEBT_TO_TOT_EQY\",\n",
        "\n",
        "    # SIZE\n",
        "    \"CURRENT_MARKET_CAP_SHARE_CLASS\",\n",
        "\n",
        "    # RISK\n",
        "    \"BETA_ADJ_OVERRIDABLE\",\n",
        "    \"VOLATILITY_30D\", \"VOLATILITY_90D\", \"VOLATILITY_180D\", \"VOLATILITY_360D\",\n",
        "\n",
        "    # TAIL RISK\n",
        "    \"RET_SKEW_30D\", \"RET_SKEW_90D\", \"RET_SKEW_180D\", \"RET_SKEW_360D\",\n",
        "    \"RET_KURT_30D\", \"RET_KURT_180D\", \"RET_KURT_360D\", \"RET_KURT_90D\",\n",
        "\n",
        "    # LIQUIDITY\n",
        "    \"TURNOVER\",\n",
        "\n",
        "    \"RET_30D\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "W6rKxTpCFN2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factors_kmeans = factors_std.loc[:, factors_std.columns.get_level_values(\"metric\").isin(keep_metrics)]"
      ],
      "metadata": {
        "id": "WVVclKhTFYpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factors_kmeans = factors_kmeans.ffill()"
      ],
      "metadata": {
        "id": "tkdZnjsRManK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(factors_kmeans)"
      ],
      "metadata": {
        "id": "ZX0L2AZNEs6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collapse K-means features: median across stocks per date\n",
        "X_kmeans = factors_kmeans.groupby(level=\"metric\", axis=1).median()\n",
        "# Collapse cross-section by taking median across stocks\n",
        "mom6_feat  = mom6_z.median(axis=1)\n",
        "\n",
        "X_mom = pd.DataFrame({\n",
        "    \"MOM_6_1\": mom6_feat,\n",
        "})\n",
        "# Combine kmeans factors + momentum\n",
        "X_all = pd.concat([X_kmeans, X_mom], axis=1)\n",
        "\n",
        "corr_all = X_all.corr()\n",
        "corr_all  # bottom rows are MOM6, MOM12 correlations\n"
      ],
      "metadata": {
        "id": "5rLb7_BSEhUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Absolute correlation\n",
        "abs_corr = corr_all.abs()\n",
        "\n",
        "# Threshold\n",
        "thr = 0.7\n",
        "\n",
        "# Get upper triangle mask (avoid duplicates & self-corr=1)\n",
        "mask = np.triu(np.ones(abs_corr.shape), k=1).astype(bool)\n",
        "\n",
        "# Extract pairs\n",
        "high_corr_pairs = (\n",
        "    abs_corr.where(mask)\n",
        "            .stack()\n",
        "            .reset_index()\n",
        ")\n",
        "\n",
        "high_corr_pairs.columns = [\"Feature1\", \"Feature2\", \"Correlation\"]\n",
        "\n",
        "# Apply threshold\n",
        "high_corr_pairs = high_corr_pairs[high_corr_pairs[\"Correlation\"] >= thr]\n",
        "\n",
        "# Sort by highest correlation\n",
        "high_corr_pairs = high_corr_pairs.sort_values(\"Correlation\", ascending=False)\n",
        "\n",
        "high_corr_pairs\n"
      ],
      "metadata": {
        "id": "2uPbiN1tF6m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def choose_k_silhouette(X, k_min=5, k_max=20):\n",
        "    best_k = k_min\n",
        "    best_score = -1\n",
        "\n",
        "    for k in range(k_min, k_max+1):\n",
        "        km = KMeans(n_clusters=k, n_init=10, random_state=0).fit(X)\n",
        "        score = silhouette_score(X, km.labels_)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "\n",
        "    return best_k\n"
      ],
      "metadata": {
        "id": "5KecR0KoX3Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# Feature smoothing (EWMA)\n",
        "\n",
        "lambda_ = 0.94\n",
        "alpha = 1 - lambda_\n",
        "\n",
        "metrics_in_data = factors_kmeans.columns.get_level_values(\"metric\").unique()\n",
        "\n",
        "for m in metrics_in_data:\n",
        "    X = factors_kmeans.xs(m, axis=1, level=\"metric\")\n",
        "    X_smooth = X.ewm(alpha=alpha, adjust=False, min_periods=1).mean()\n",
        "\n",
        "    X_smooth.columns = pd.MultiIndex.from_product(\n",
        "        [[m], X_smooth.columns], names=[\"metric\", \"stock\"]\n",
        "    )\n",
        "\n",
        "    mask = factors_kmeans.columns.get_level_values(\"metric\") == m\n",
        "    factors_kmeans.loc[:, mask] = X_smooth.values\n",
        "\n",
        "# Monthly snapshot\n",
        "factors_kmeans_m = factors_kmeans.resample(\"M\").last()\n",
        "\n",
        "\n",
        "# Momentum signal (6-1)\n",
        "\n",
        "def winsorize_cs(row, lower=0.01, upper=0.99):\n",
        "    if row.isna().all():\n",
        "        return row\n",
        "    lo, hi = row.quantile([lower, upper])\n",
        "    return row.clip(lo, hi)\n",
        "\n",
        "mom6_w = mom_6_1.apply(winsorize_cs, axis=1)\n",
        "\n",
        "def zscore_cs(X):\n",
        "    return X.sub(X.mean(axis=1), axis=0).div(X.std(axis=1).replace(0, np.nan), axis=0)\n",
        "\n",
        "mom6_z = zscore_cs(mom6_w)\n",
        "\n",
        "# Forward monthly return\n",
        "fwd_ret_m = np.log(px_m.shift(-1)) - np.log(px_m)\n",
        "\n",
        "\n",
        "\n",
        "# Long-short return function\n",
        "def long_short_return(signal_cs, fwd_ret_cs, long_frac=0.2, short_frac=0.2):\n",
        "    sig = signal_cs.dropna()\n",
        "    common = sig.index.intersection(fwd_ret_cs.dropna().index)\n",
        "    if len(common) < 10:\n",
        "        return np.nan\n",
        "\n",
        "    sig = sig.loc[common]\n",
        "    r = fwd_ret_cs.loc[common]\n",
        "\n",
        "    n = len(sig)\n",
        "    n_long = max(1, int(n * long_frac))\n",
        "    n_short = max(1, int(n * short_frac))\n",
        "\n",
        "    sig_sorted = sig.sort_values()\n",
        "\n",
        "    long_names = sig_sorted.index[-n_long:]\n",
        "    short_names = sig_sorted.index[:n_short]\n",
        "\n",
        "    return r.loc[long_names].mean() - r.loc[short_names].mean()\n",
        "\n",
        "\n",
        "# Rolling K-means backtest\n",
        "K = 30 # number of clusters\n",
        "\n",
        "# Valid monthly dates\n",
        "rebalance_dates = mom6_z.dropna(how=\"all\").index\n",
        "rebalance_dates = rebalance_dates.intersection(factors_kmeans_m.index)\n",
        "rebalance_dates = rebalance_dates.intersection(fwd_ret_m.index)\n",
        "\n",
        "rebalance_dates = rebalance_dates[(rebalance_dates >= \"2011-01-31\") &\n",
        "                                  (rebalance_dates <= \"2020-12-31\")]\n",
        "\n",
        "baseline_returns = []\n",
        "cluster_returns = []\n",
        "cluster_dates = []\n",
        "\n",
        "# Baseline\n",
        "for t in rebalance_dates[:-1]:\n",
        "    baseline_returns.append(long_short_return(mom6_z.loc[t], fwd_ret_m.loc[t]))\n",
        "# full-df baseline dates (no intersection with cluster dates)\n",
        "baseline_dates = rebalance_dates[:-1]\n",
        "\n",
        "# Clustered stratege\n",
        "\n",
        "for t in rebalance_dates[:-1]:\n",
        "    t_next = rebalance_dates[rebalance_dates.get_loc(t) + 1]\n",
        "\n",
        "    # Load monthly features\n",
        "    row_t = factors_kmeans_m.loc[t]\n",
        "    X_t = row_t.unstack(\"metric\")\n",
        "\n",
        "    # Keep stocks with >=70% non-NaN features\n",
        "    min_valid = int(0.7 * X_t.shape[1])\n",
        "    X_t = X_t.dropna(axis=0, thresh=min_valid).fillna(0)\n",
        "\n",
        "    if X_t.shape[0] < K + 5:\n",
        "        continue\n",
        "\n",
        "    # Fit K-means\n",
        "    km = KMeans(n_clusters=K, n_init=50, random_state=0)\n",
        "    labels = km.fit_predict(X_t.values)\n",
        "\n",
        "    clusters_t = pd.Series(labels, index=X_t.index, name=\"cluster\")\n",
        "\n",
        "    sig6_t = mom6_z.loc[t]\n",
        "    fwd_t = fwd_ret_m.loc[t]\n",
        "\n",
        "    # Align to available universe\n",
        "    universe = X_t.index.intersection(sig6_t.dropna().index).intersection(fwd_t.dropna().index)\n",
        "    if len(universe) < K * 5:\n",
        "        continue\n",
        "\n",
        "    clusters_t = clusters_t.loc[universe]\n",
        "    sig6_t = sig6_t.loc[universe]\n",
        "    fwd_t = fwd_t.loc[universe]\n",
        "\n",
        "    # Drop clusters with <5 names\n",
        "    valid_clusters = clusters_t.value_counts()[lambda x: x >= 5].index\n",
        "    clusters_t = clusters_t[clusters_t.isin(valid_clusters)]\n",
        "    sig6_t = sig6_t.loc[clusters_t.index]\n",
        "    fwd_t = fwd_t.loc[clusters_t.index]\n",
        "\n",
        "    # Equal-weight long/short across clusters\n",
        "    cluster_ls = []\n",
        "    for cid in clusters_t.unique():\n",
        "        idx = clusters_t.index[clusters_t == cid]\n",
        "        if len(idx) < 10:\n",
        "            continue\n",
        "        ret_c = long_short_return(sig6_t.loc[idx], fwd_t.loc[idx])\n",
        "        if not np.isnan(ret_c):\n",
        "            cluster_ls.append(ret_c)\n",
        "\n",
        "    if len(cluster_ls) < max(3, int(0.3 * K)):\n",
        "        continue\n",
        "\n",
        "    cluster_returns.append(np.mean(cluster_ls))\n",
        "    cluster_dates.append(t_next)\n",
        "\n",
        "\n",
        "# Build results dataframe\n",
        "\n",
        "results = pd.DataFrame(index=baseline_dates)\n",
        "results[\"Baseline_6_1\"] = baseline_returns\n",
        "results[\"Cluster_6_1\"] = pd.Series(cluster_returns, index=cluster_dates)\n",
        "\n",
        "cum = (1 + results).cumprod()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cum.index, cum[\"Baseline_6_1\"], label=\"Baseline 6-1\")\n",
        "plt.plot(cum.index, cum[\"Cluster_6_1\"], label=\"Clustered 6-1\")\n",
        "plt.legend()\n",
        "plt.title(\"Cumulative Returns: Baseline vs Clustered Momentum (Dollar-Neutral)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jBEEeqhHGf6S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}