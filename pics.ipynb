{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 0. Imports & Colab setup\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# ============================================\n",
    "# 1. Load data from MyDrive\n",
    "# ============================================\n",
    "\n",
    "metric_data = \"E:\\\\5 Code\\\\2025_cu_qmim\\\\data\\\\price_metrics.parquet\"\n",
    "std_data    = \"E:\\\\5 Code\\\\2025_cu_qmim\\\\data\\\\factors_std.parquet\"\n",
    "\n",
    "px_all      = pd.read_parquet(metric_data)\n",
    "factors_std = pd.read_parquet(std_data)\n",
    "\n",
    "# Use PX_LAST only (MultiIndex with level \"metric\")\n",
    "px = px_all.xs(\"PX_LAST\", axis=1, level=\"metric\")\n",
    "\n",
    "# Monthly prices (month end)\n",
    "px_m = px.resample(\"M\").last()\n",
    "\n",
    "# ============================================\n",
    "# 2. Factor set for K-means\n",
    "# ============================================\n",
    "keep_metrics = [\n",
    "    # VALUE\n",
    "    \"PE_RATIO\", \"PX_TO_BOOK_RATIO\", \"PX_TO_SALES_RATIO\",\n",
    "    \"CURRENT_EV_TO_T12M_EBITDA\", \"FREE_CASH_FLOW_YIELD\",\n",
    "    \"EQY_DVD_YLD_12M\",\n",
    "\n",
    "    # QUALITY\n",
    "    \"EBITDA_MARGIN\", \"GROSS_MARGIN\", \"OPER_MARGIN\",\n",
    "    \"PROF_MARGIN\", \"RETURN_ON_ASSET\",\n",
    "\n",
    "    # LEVERAGE\n",
    "    \"TOT_DEBT_TO_EBITDA\", \"TOT_DEBT_TO_TOT_EQY\",\n",
    "\n",
    "    # SIZE\n",
    "    \"CURRENT_MARKET_CAP_SHARE_CLASS\",\n",
    "\n",
    "    # RISK\n",
    "    \"BETA_ADJ_OVERRIDABLE\", \"VOLATILITY_30D\", \"VOLATILITY_90D\",\n",
    "    \"VOLATILITY_180D\", \"VOLATILITY_360D\",\n",
    "\n",
    "    # TAIL RISK\n",
    "    \"RET_SKEW_30D\", \"RET_SKEW_90D\", \"RET_SKEW_180D\", \"RET_SKEW_360D\",\n",
    "    \"RET_KURT_30D\", \"RET_KURT_180D\", \"RET_KURT_360D\", \"RET_KURT_90D\",\n",
    "\n",
    "    # LIQUIDITY / MOM\n",
    "    \"TURNOVER\", \"RET_30D\",\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# 3. Helpers: cleaning & EWMA\n",
    "# ============================================\n",
    "def _winsorize_row(row, lower, upper):\n",
    "    if row.isna().all():\n",
    "        return row\n",
    "    lo, hi = row.quantile([lower, upper])\n",
    "    return row.clip(lo, hi)\n",
    "\n",
    "def clean_data(df, lower=0.01, upper=0.99):\n",
    "    # cross-sectional winsorization\n",
    "    df_w = df.apply(_winsorize_row, axis=1, args=(lower, upper))\n",
    "    # cross-sectional z-score\n",
    "    mean_cs = df_w.mean(axis=1)\n",
    "    std_cs  = df_w.std(axis=1).replace(0, np.nan)\n",
    "    df_z    = df_w.sub(mean_cs, axis=0).div(std_cs, axis=0)\n",
    "    return df_z\n",
    "\n",
    "def ewma(factors_kmeans, lambda_=0.94):\n",
    "    alpha = 1 - lambda_\n",
    "    metrics_in_data = factors_kmeans.columns.get_level_values(\"metric\").unique()\n",
    "\n",
    "    for m in metrics_in_data:\n",
    "        X = factors_kmeans.xs(m, axis=1, level=\"metric\")\n",
    "        X_smooth = X.ewm(alpha=alpha, adjust=False, min_periods=1).mean()\n",
    "        # put back metric level\n",
    "        X_smooth.columns = pd.MultiIndex.from_product(\n",
    "            [[m], X_smooth.columns], names=[\"metric\", \"stock\"]\n",
    "        )\n",
    "        mask = factors_kmeans.columns.get_level_values(\"metric\") == m\n",
    "        factors_kmeans.loc[:, mask] = X_smooth.values\n",
    "\n",
    "    return factors_kmeans\n",
    "\n",
    "# ============================================\n",
    "# 4. Build momentum, factors, trade dates\n",
    "# ============================================\n",
    "def get_trade_setup(px_m, factors_std, gap=6):\n",
    "    # Forward 1M log returns\n",
    "    fwd_ret_m = np.log(px_m.shift(-1)) - np.log(px_m)\n",
    "\n",
    "    # \"6-1\"-style momentum with gap:\n",
    "    # mom[t] = log(P[t-1]) - log(P[t-gap-1])\n",
    "    mom_raw = np.log(px_m.shift(1)) - np.log(px_m.shift(gap + 1))\n",
    "\n",
    "    # Clean & shift by \"gap\" to avoid look-ahead\n",
    "    mom_z = clean_data(mom_raw.shift(gap))\n",
    "\n",
    "    # Subset factor metrics for K-means\n",
    "    factors_kmeans = factors_std.loc[\n",
    "        :, factors_std.columns.get_level_values(\"metric\").isin(keep_metrics)\n",
    "    ]\n",
    "    factors_kmeans = factors_kmeans.ffill()\n",
    "    factors_kmeans_m = ewma(factors_kmeans)\n",
    "\n",
    "    # Dates where all needed pieces exist\n",
    "    rebalance_dates = mom_z.dropna(how=\"all\").index\n",
    "    rebalance_dates = rebalance_dates.intersection(factors_kmeans_m.index)\n",
    "    rebalance_dates = rebalance_dates.intersection(fwd_ret_m.index)\n",
    "\n",
    "    # Sample window\n",
    "    rebalance_dates = rebalance_dates[\n",
    "        (rebalance_dates >= \"2011-01-31\") & (rebalance_dates <= \"2020-12-31\")\n",
    "    ]\n",
    "\n",
    "    return rebalance_dates, factors_kmeans_m, mom_z, fwd_ret_m\n",
    "\n",
    "gap = 4\n",
    "rebalance_dates, factors_kmeans_m, mom_z, fwd_ret_m = get_trade_setup(\n",
    "    px_m, factors_std, gap=gap\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 5. K-means clusters (K = 30)\n",
    "# ============================================\n",
    "def get_clusters(rebalance_dates, factors_kmeans_m, K=30):\n",
    "    clusters_dict = {}\n",
    "    for t in rebalance_dates:\n",
    "        row_t = factors_kmeans_m.loc[t]           # row: MultiIndex (metric, stock)\n",
    "        X_t = row_t.unstack(\"metric\")            # index: stock, columns: metric\n",
    "\n",
    "        # require at least 70% of metrics per stock\n",
    "        min_valid = int(0.7 * X_t.shape[1])\n",
    "        X_t = X_t.dropna(axis=0, thresh=min_valid).fillna(0)\n",
    "\n",
    "        if X_t.shape[0] < K:\n",
    "            # not enough stocks, skip this date\n",
    "            continue\n",
    "\n",
    "        km = KMeans(n_clusters=K, n_init=50, random_state=0)\n",
    "        labels = km.fit_predict(X_t.values)\n",
    "\n",
    "        clusters_t = pd.Series(labels, index=X_t.index, name=\"cluster\")\n",
    "        clusters_dict[t] = clusters_t\n",
    "\n",
    "    return clusters_dict\n",
    "\n",
    "K = 30\n",
    "clusters_dict = get_clusters(rebalance_dates, factors_kmeans_m, K=K)\n",
    "\n",
    "# Keep only dates where we actually got clusters\n",
    "rebalance_dates = pd.Index(clusters_dict.keys()).sort_values()\n",
    "\n",
    "# ============================================\n",
    "# 6. Clustered momentum signal (equal-weight LS within cluster)\n",
    "# ============================================\n",
    "def build_cluster_mom_signal_equal(rebalance_dates, clusters_dict, mom_z,\n",
    "                                   top_per=0.2, bottom_per=0.2):\n",
    "\n",
    "    all_stocks = mom_z.columns\n",
    "    signal_list = []\n",
    "\n",
    "    for t in rebalance_dates:\n",
    "        clusters_t = clusters_dict[t]\n",
    "        mom_t = mom_z.loc[t]\n",
    "\n",
    "        sig_t = pd.Series(0.0, index=all_stocks)\n",
    "\n",
    "        for cl in clusters_t.unique():\n",
    "            in_cl = clusters_t[clusters_t == cl].index\n",
    "            mom_cl = mom_t.reindex(in_cl).dropna()\n",
    "            if len(mom_cl) < 5:\n",
    "                continue\n",
    "\n",
    "            n_top = max(1, int(len(mom_cl) * top_per))\n",
    "            n_bot = max(1, int(len(mom_cl) * bottom_per))\n",
    "\n",
    "            top_idx = mom_cl.nlargest(n_top).index\n",
    "            bot_idx = mom_cl.nsmallest(n_bot).index\n",
    "\n",
    "            sig_t.loc[top_idx] = 1.0\n",
    "            sig_t.loc[bot_idx] = -1.0\n",
    "\n",
    "        sig_t.name = t\n",
    "        signal_list.append(sig_t)\n",
    "\n",
    "    signal_df = pd.DataFrame(signal_list)\n",
    "    signal_df.index.name = \"date\"\n",
    "    return signal_df\n",
    "\n",
    "cluster_mom_sig_equal = build_cluster_mom_signal_equal(\n",
    "    rebalance_dates, clusters_dict, mom_z, top_per=0.2, bottom_per=0.2\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 7. Clustered momentum signal (momentum-weighted LS within cluster)\n",
    "# ============================================\n",
    "def build_cluster_mom_signal_weighted(rebalance_dates, clusters_dict, mom_z,\n",
    "                                      top_per=0.2, bottom_per=0.2):\n",
    "    \"\"\"\n",
    "    Within each cluster:\n",
    "    - select top_per and bottom_per by momentum\n",
    "    - long weights ∝ positive momentum\n",
    "    - short weights ∝ negative momentum (more negative → larger short)\n",
    "    Result is already dollar-neutral per cluster in spirit; final portfolio\n",
    "    is normalized later anyway.\n",
    "    \"\"\"\n",
    "    all_stocks = mom_z.columns\n",
    "    signal_list = []\n",
    "\n",
    "    for t in rebalance_dates:\n",
    "        clusters_t = clusters_dict[t]\n",
    "        mom_t = mom_z.loc[t]\n",
    "\n",
    "        w_t = pd.Series(0.0, index=all_stocks)\n",
    "\n",
    "        for cl in clusters_t.unique():\n",
    "            in_cl = clusters_t[clusters_t == cl].index\n",
    "            mom_cl = mom_t.reindex(in_cl).dropna()\n",
    "            if len(mom_cl) < 5:\n",
    "                continue\n",
    "\n",
    "            n_top = max(1, int(len(mom_cl) * top_per))\n",
    "            n_bot = max(1, int(len(mom_cl) * bottom_per))\n",
    "\n",
    "            top = mom_cl.nlargest(n_top)\n",
    "            bot = mom_cl.nsmallest(n_bot)\n",
    "\n",
    "            # Long weights proportional to positive momentum\n",
    "            if top.abs().sum() > 0:\n",
    "                w_long = top / top.abs().sum()\n",
    "            else:\n",
    "                w_long = pd.Series(0.0, index=top.index)\n",
    "\n",
    "            # Short weights proportional to |negative momentum|\n",
    "            # End result: negative weights for shorts\n",
    "            if bot.abs().sum() > 0:\n",
    "                w_short = -bot.abs() / bot.abs().sum()\n",
    "            else:\n",
    "                w_short = pd.Series(0.0, index=bot.index)\n",
    "\n",
    "            w_cluster = pd.concat([w_long, w_short])\n",
    "\n",
    "            # Assign to overall vector\n",
    "            w_t.loc[w_cluster.index] += w_cluster\n",
    "\n",
    "        w_t.name = t\n",
    "        signal_list.append(w_t)\n",
    "\n",
    "    signal_df = pd.DataFrame(signal_list)\n",
    "    signal_df.index.name = \"date\"\n",
    "    return signal_df\n",
    "\n",
    "cluster_mom_sig_weighted = build_cluster_mom_signal_weighted(\n",
    "    rebalance_dates, clusters_dict, mom_z, top_per=0.2, bottom_per=0.2\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 8. Basic momentum signal (no clustering, equal-weight LS)\n",
    "# ============================================\n",
    "def build_basic_mom_signal(mom_df, dates, long_q=0.8, short_q=0.2):\n",
    "    sig_list = []\n",
    "    all_stocks = mom_df.columns\n",
    "\n",
    "    for dt in dates:\n",
    "        row = mom_df.loc[dt]\n",
    "        r = row.rank(pct=True)\n",
    "\n",
    "        sig = pd.Series(0.0, index=all_stocks)\n",
    "        sig[r >= long_q] = 1.0\n",
    "        sig[r <= short_q] = -1.0\n",
    "        sig.name = dt\n",
    "\n",
    "        sig_list.append(sig)\n",
    "\n",
    "    signal_df = pd.DataFrame(sig_list)\n",
    "    signal_df.index.name = \"date\"\n",
    "    return signal_df\n",
    "\n",
    "basic_mom_sig = build_basic_mom_signal(mom_z, rebalance_dates,\n",
    "                                       long_q=0.8, short_q=0.2)\n",
    "\n",
    "# ============================================\n",
    "# 9. IC series for all three strategies\n",
    "# ============================================\n",
    "def calc_ic_series(signal_df, fwd_ret_df, dates):\n",
    "    ic_list = []\n",
    "    for dt in dates:\n",
    "        sig = signal_df.loc[dt]\n",
    "        ret = fwd_ret_df.loc[dt]\n",
    "\n",
    "        mask = sig.notna() & ret.notna()\n",
    "        if mask.sum() < 5:\n",
    "            ic = np.nan\n",
    "        else:\n",
    "            ic, _ = spearmanr(sig[mask], ret[mask])\n",
    "        ic_list.append(ic)\n",
    "\n",
    "    return pd.Series(ic_list, index=dates)\n",
    "\n",
    "ic_basic    = calc_ic_series(basic_mom_sig,              fwd_ret_m, rebalance_dates)\n",
    "ic_cluster  = calc_ic_series(cluster_mom_sig_equal,      fwd_ret_m, rebalance_dates)\n",
    "ic_clust_wt = calc_ic_series(cluster_mom_sig_weighted,   fwd_ret_m, rebalance_dates)\n",
    "\n",
    "# ============================================\n",
    "# 10. Convert signals to daily returns & cum returns\n",
    "# ============================================\n",
    "def signal_to_daily_returns(signal_df, px):\n",
    "    \"\"\"\n",
    "    signal_df: index = rebalance_dates (monthly), values: weights or signals\n",
    "    px       : daily price dataframe\n",
    "    \"\"\"\n",
    "    daily_ret = px.pct_change()\n",
    "\n",
    "    # Normalize each rebalance date: sum |w_i| = 1 (dollar-neutral gross)\n",
    "    w = signal_df.copy()\n",
    "    gross = w.abs().sum(axis=1).replace(0, np.nan)\n",
    "    w = w.div(gross, axis=0)\n",
    "\n",
    "    # Align to daily index, forward-fill\n",
    "    w_daily = w.reindex(daily_ret.index, method=\"ffill\").fillna(0)\n",
    "\n",
    "    # Portfolio daily returns\n",
    "    port_ret = (w_daily * daily_ret).sum(axis=1)\n",
    "\n",
    "    # Start after first rebalance date\n",
    "    first_reb = signal_df.index.min()\n",
    "    port_ret = port_ret[port_ret.index >= first_reb]\n",
    "\n",
    "    return port_ret\n",
    "\n",
    "ret_basic    = signal_to_daily_returns(basic_mom_sig,            px)\n",
    "ret_cluster  = signal_to_daily_returns(cluster_mom_sig_equal,    px)\n",
    "ret_clust_wt = signal_to_daily_returns(cluster_mom_sig_weighted, px)\n",
    "\n",
    "cum_basic    = (1 + ret_basic).cumprod()\n",
    "cum_cluster  = (1 + ret_cluster).cumprod()\n",
    "cum_clust_wt = (1 + ret_clust_wt).cumprod()\n",
    "\n",
    "# ============================================\n",
    "# 11. Plot IC over time (three lines)\n",
    "# ============================================\n",
    "plt.figure()\n",
    "plt.plot(ic_basic.index,    ic_basic.values,\n",
    "         label=\"Basic Momentum (gap=4)\", alpha=0.7)\n",
    "plt.plot(ic_cluster.index,  ic_cluster.values,\n",
    "         label=\"Clustered Equal-Weight (K=30)\", alpha=0.7)\n",
    "plt.plot(ic_clust_wt.index, ic_clust_wt.values,\n",
    "         label=\"Clustered Momentum-Weighted (K=30)\", alpha=0.7)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.title(\"Information Coefficient Over Time\\nBasic vs Clustered (Equal vs Momentum-Weighted)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Spearman Rank IC\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Optional: 12M rolling IC\n",
    "ic_basic_roll    = ic_basic.rolling(12).mean()\n",
    "ic_cluster_roll  = ic_cluster.rolling(12).mean()\n",
    "ic_clust_wt_roll = ic_clust_wt.rolling(12).mean()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ic_basic_roll.index,    ic_basic_roll.values,\n",
    "         label=\"Basic Momentum 12M Rolling IC\", linewidth=2)\n",
    "plt.plot(ic_cluster_roll.index,  ic_cluster_roll.values,\n",
    "         label=\"Clustered Equal-Weight 12M Rolling IC\", linewidth=2)\n",
    "plt.plot(ic_clust_wt_roll.index, ic_clust_wt_roll.values,\n",
    "         label=\"Clustered Momentum-Weighted 12M Rolling IC\", linewidth=2)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.title(\"12-Month Rolling IC\\nBasic vs Clustered (Equal vs Momentum-Weighted)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Rolling Spearman IC\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 12. Plot cumulative returns (three lines)\n",
    "# ============================================\n",
    "plt.figure()\n",
    "plt.plot(cum_basic.index,    cum_basic.values,\n",
    "         label=\"Basic Momentum (gap=4)\")\n",
    "plt.plot(cum_cluster.index,  cum_cluster.values,\n",
    "         label=\"Clustered Equal-Weight (K=30)\")\n",
    "plt.plot(cum_clust_wt.index, cum_clust_wt.values,\n",
    "         label=\"Clustered Momentum-Weighted (K=30)\")\n",
    "plt.title(\"Cumulative Returns\\nBasic vs Clustered (Equal vs Momentum-Weighted)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return (× initial capital)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"Mean IC - Basic              :\", ic_basic.mean())\n",
    "print(\"Mean IC - Cluster Equal      :\", ic_cluster.mean())\n",
    "print(\"Mean IC - Cluster Weighted   :\", ic_clust_wt.mean())\n",
    "print(\"IC corr (Basic, Cluster Eq)  :\", ic_basic.corr(ic_cluster))\n",
    "print(\"IC corr (Basic, Cluster Wt)  :\", ic_basic.corr(ic_clust_wt))\n",
    "print(\"IC corr (Cluster Eq, Wt)     :\", ic_cluster.corr(ic_clust_wt))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
